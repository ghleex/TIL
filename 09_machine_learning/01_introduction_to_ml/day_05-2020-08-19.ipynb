{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 비지도학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 군집화\n",
    "* 연관규칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. 군집화(clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 비슷한 것들을 찾아 그룹을 만드는 것\n",
    "* 분류(classification)와 혼동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 물건을 정리할 때 비슷한 것들을 모아 그룹 만드는 것: 군집화\n",
    "* 어떤 대상이 어떤 그룹에 속하는지 판단: 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1000만 명 사용자 관리 위한 100개 그룹 만들기\n",
    "  * 좌표평면을 이용하여 적절히 분포되어 있는 100개 그룹을 만들 수 있음\n",
    "  * 하지만 열이 100개, 1000개 등 많아진다면 단순히 좌표평면에 표현하는 것은 불가능\n",
    "  * 이때 군집화를 이용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 군집화는 서로 가까운 관측치를 찾아주는 머신러닝 기법\n",
    "  * 좌표 상 가깝다: 데이터가 서로 비슷하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 연관규칙학습(association rule learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 서로 연관된 특징을 찾아내는 것\n",
    "* a.k.a. 장바구니 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 장바구니에 담긴 상품을 이용하여 관심 상품 추천\n",
    "* 예를 들어 라면과 계란을 동시에 구입한 경우가 많다면 라면과 계란은 서로 연관성이 높다고 볼 수 있음\n",
    "* 판매 제품 종류가 1만 개이고, 하루 1000만 명이 쇼핑몰을 이용한다면 연관성을 끌어내기는 쉽지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 쇼핑 추천, 음악 추천, 영화 추천, 검색어 추천, 동영상 추천 등이 이에 해당"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 서로 관련 있는 특성(열; column)을 찾아주는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 관측치(행) groupping: 군집화(clustering)\n",
    "* 특성(열) groupping: 연관규칙(association rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. 비지도학습 vs. 지도학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 비지도학습: 탐험 - 데이터 성격 파악이 목적\n",
    "* 지도학습: 역사 - 과거의 원인과 결과를 바탕으로 결과를 모르는 원인 발생 시 결과 예측이 목적\n",
    "  * 따라서 독립변수, 종속변수가 반드시 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 강화학습(reinforcement learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 지도학습: 배움을 통해 실력 키우기\n",
    "* 강화학습: 경험을 통해 실력 키우기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 경험의 결과가 유리한 것이면 보상을, 불리한 것이면 벌을 주는 방식\n",
    "* 이 과정을 무수히 많이 반복하면 더 많은 보상을 받기 위한 좋은 답을 찾아낼 수 있다는 것이 강화학습의 기본 아이디어임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 강화학습에서는 \"더 많은 보상을 받을 수 있는 정책\"을 만드는 것이 핵심임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 강화학습을 통해 구현된 예\n",
    "  * 알파고\n",
    "  * 자동차 자율주행 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 게임과 강화학습을 같이 보면?\n",
    "\n",
    "| 게임 | 강화학습 |\n",
    "|------|------|\n",
    "| 게임 | 환경(environment) |\n",
    "| 게이머 | 에이전트(agent) |\n",
    "| 게임환경 | 상태(state) |\n",
    "| 게이머의 조작 | 행동(action) |\n",
    "| 상과 벌 | 보상(reward) |\n",
    "| 게이머의 판단력 | 정책(policy) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 강화학습의 목적: 상태에 따라 더 많은 보상을 받을 수 있는 행동을 에이전트가 할 수 있도록 하는 정책을 만드는 것"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit41c0f892f0d2444caf8d56aed2f67eea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
