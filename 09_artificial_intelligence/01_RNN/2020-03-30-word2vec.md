### 2020-03-30

# word2vec

## 추론 기반 기법과 신경망

### 통계 기반 기법의 문제점

* 통계 기반 기법에서는 주변 단어의 빈도를 기초로 단어를 표현
* 대규모 말뭉치를 다룰 때 문제 발생
* 말뭉치의 전체 통계(동시 발생 행렬과 PPMI 등)를 이용하여 단 1회의 처리(SVD 등)만에 단어의 분산 표현을 얻음
* 추론 기반 기법에서는 신경망을 이용하는 경우 미니배치로 학습하는 것이 일반적
* 미니배치 학습에서는 신경망이 한 번에 소량(미니배치)의 학습 샘플씩 반복하여 학습하며 가중치를 갱신

#### 통계 기반 기법과 추론 기반 기법 비교

* 통계 기반 기법: 학습 데이터를 한꺼번에 처리(배치 학습)
* 추론 기반 기법: 학습 데이터 일부를 사용하여 순차적으로 학습(미니배치 학습)
  * 말뭉치의 어휘 수가 많아 SVD 등 계산량이 큰 작업을 처리하기 어려운 경우에도 신경망 학습 가능
  * 여러 GPU 를 이용한 병렬 계산도 가능해져 학습 속도를 높일 수 있음



### 추론 기반 기법 개요

* 주변 어휘(맥락)가 주어져씅ㄹ 때 "?"에 어떤 어휘가 들어갈지 추측하는 작업
  * you __?__ goodbye and I say hello.
* 추론 기반 기법에서는 모델로 신경망을 사용
* 모델은 맥락 정보를 입력받아 (출현할 수 있는) 단어들의 출현 확률을 출력
* 이러한 틀 속에서 말뭉치를 사용하여 모델이 올바른 추측을 내놓도록 학습
* 그 학습 결과로 단어의 분산 표현을 얻는 것이 추론 기반 기법의 전체 그림



### 신경망에서의 단어 처리

* 신경망은 "you" 와 "say" 등의 단어를 있는 그대로 처리할 수 없으니 단어를 "고정 길이의 벡터"로 변환해야 함

* 이때 사용하는 대표적인 방법: 단어를 원핫(one-hot) 표현(또는 원핫 벡터)으로 변환하는 것

  * 원핫 표현: 벡터의 원소 중 하나만 1이고 나머지 모두는 0인 것

* 단어를 원핫 표현으로 변환하는 방법

  * 총 어휘 수만큼의 원소를 갖는 벡터를 준비
  * 인덱스가 단어 ID 와 같은 원소를 1로, 나머지는 모두 0으로 설정

* 단어를 벡터로 나타낼 수 있고, 신경망을 구성하는 "계층"들은 벡터 처리 가능. 다시 말해 단어를 신경망으로 처리 가능

* 완전연결계층: 각 노드가 이웃 층의 모든 노드와 화살표로 연결

  * 화살표에는 가중치(매개변수)가 존재
  * 입력층 뉴런과의 가중합이 은닉층 뉴런이 됨

  ```python
  import numpy as np
  
  c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # 입력
  W = np.random.randn(7, 3) # 가중치
  h = np.matmul(c, W)
  print(h)
  # [[-0.49356447 -0.42173813 -1.18148224]]
  ```

* 완전연결계층의 계산은 행렬 곱으로 수행 가능

  * 행렬 곱은 `np.matmul()`





## 단순한 word2vec

* CBOW(continuous bag-of-words) 모델

### CBOW 모델의 추론 처리

* CBOW 모델은 맥락으로부터 타깃을 추측하는 용도의 신경망
* CBOW 모델의 입력은 맥락
* 맥락을 원핫 표현으로 변환하여 CBOW 모델이 처리할 수 있도록 함
* 학습을 진행할수록 맥락에서 출현하는 단어를 잘 추측하는 방향으로 분산 표현들이 갱신됨
* 과정
  * 필요한 가중치 초기화
  * 입력층 처리 계층을 맥락 수만큼 생성(가중치 공유)
  * 출력층 측의 계층은 1개만 생성



### CBOW 모델의 학습

* CBOW 모델의 출력층에서 단어의 점수 출력
* 이 점수에 소프트맥스 적용 시 확률을 얻을 수 있음
* 이 확률은 맥락이 주어졌을 때 그 중앙에 어떤 단어가 출현하는지 나타냄
* 가중치가 적절히 설정된 신경망인 경우 "확률"을 나타내는 뉴런들 중 정답에 해당하는 뉴런의 값이 가장 클 것으로 예상 가능
* CBOW 모델의 학습에서는 올바른 예측을 할 수 있도록 가중치를 조정하는 일을 함
* 그 결과 가중치에 단어의 출현 패턴을 파악한 벡터가 학습
* CBOW 모델은 단어 출현 패턴을 학습 시 사용한 말뭉치에서 배움
* 따라서 말뭉치가 달라지면 학습 후 얻게되는 단어의 분산 표현도 달라짐
* 다중 클래스 분류 수행 신경망 -- 학습을 위하여 소프트맥스와 교차 엔트로피 오차만 이용하면 됨
  * 소프트맥스 함수를 이용, 점수를 확률로 변환
  * 확률과 정답 레이블로부터 교차 엔트로피 오차를 구함
  * 그 오차를 손실로 사용하여 학습 진행



### word2vec 의 가중치와 분산 표현

* word2vec 에서 사용되는 신경망의 두 가지 가중치
  * 입력 측 완전연결계층의 가중치: 각 행이 각 어휘의 분산 표현
  * 출력 측 완전연결계층의 가중치: 어휘의 의미가 인코딩된 벡터가 저장. 어휘의 분산 표현이 열 방향(수직 방향)으로 저장

