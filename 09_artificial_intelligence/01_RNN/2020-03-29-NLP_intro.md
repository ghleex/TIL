### 2020-03-29

# 자연어와 단어의 분산 표현

## 자연어 처리란

* 자연어(natural language): 사람들이 일상적으로 사용하는 언어
* 자연어 처리(natural language processing): 자연어를 처리하는 분야, 인간 언어를 컴퓨터에게 이해시키기 위한 기술(분야)
* 자연어 처리가 추구하는 목표: 인간 언어를 컴퓨터가 이해하도록 만들어 컴퓨터가 우리에게 도움이 되는 일을 수행하도록 하는 것

* 인간 언어는 "부드러운 언어"임
  * '부드럽다': 같은 의미의 문장이 여러 형태로 표현되거나 문장 의미가 모호성을 가지거나 그 의미 또는 형태가 유연하게 바뀐다는 것
* Examples: 검색 엔진, 기계 번역, IBM 의 Watson



### 단어의 의미

* 주제: 컴퓨터에게 단어 이해시키기
* 말은 문자로 구성, 말의 의미는 "단어"로 구성
* 따라서 단어는 의미의 최소 단위라 볼 수 있음
* 결국 자연어(인간 언어)를 컴퓨터에게 이해시키는 데 있어 "단어의 의미"를 이해시키는 것이 중요
* 앞으로 살펴볼 것
  * 시소러스(thesaurus) 를 활용한 기법
  * 통계 기반 기법
  * 추론 기반 기법(word2vec)





## 시소러스(thesaurus)

* "단어의 의미"를 나타내는 방법으로 사람이 직접 단어의 의미를 정의하는 방식을 생각할 수 있음
* 표준국어대사전에서와 같이 개별 단어에 그 의미를 설명해 넣는 것
* 자연어 처리에서도 단어의 의미를 인력을 동원해 정의하려는 시도는 수없이 있어왔음
* 단, 시소러스(thesaurus) 형태의 사전을 애용
  * 시소러스: 유의어 사전, 뜻이 "같은 단어(동의어)"나 "유사한 단어(유의어)" 가 한 그룹으로 분류
* 자연어 처리에 이용되는 시소러스에는 단어 사이의 "상위와 하위" 또는 "전체와 부분" 등 더 세세한 관계까지 정의해둔 경우가 있음
* 모든 단어에 대한 유의어 집합을 만든 후 단어 간 관계를 그래프로 표현하여 단어 사이의 연결을 정의



### WordNet

* WordNet: 자연어 처리 분야에서 가장 유명한 시소러스
* WordNet 을 사용하여 유의어를 얻거나 "단어 네트워크" 활용 가능



### 시소러스의 문제점

* 시대 변화에 대응하기 어려움: 신조어 또는 어휘의 의미 변화 등에 즉각적인 대응이 어려움
* 사람을 쓰는 비용이 큼: 시소러스 만드는 데 엄청난 비용 발생
  * 현존 영어 단어 수: 1,000 만 개 이상
  * WordNet 등록 어휘: 20 만개 이상
* 단어가 가지는 미묘한 차이를 표현할 수 없음
  * vintage vs. retro
* 사람의 개입을 최소로 줄이고 텍스트 데이터만으로 원하는 결과를 얻어내는 방향으로 패러다임이 변화하고 있음





## 통계 기반 기법

* 말뭉치(corpus) 활용 방법
* 말뭉치?
  * Biber, et al. (1998): 기존 언어학 연구는 언어 구조 분석에 초점을 두었으나 코퍼스 언어학은 언어 사용을 연구하기 위한 것으로, 특정 어휘 구조 또는 빈도, 분포 등을 살펴보기 위한 연구 방법
  * McEnery and Hardie (2012): 언어 연구 방법에 초점을 두지만, 언어 탐구를 위해 모든 학자들이 동의한 단일한 방법이나 과정은 아님. 즉, 연구시 필요에 따라 사용될 수 있는 방법론 중 하나
  * Sinclair (2004): 언어 연구를 위한 데이터 자료로 외적 기준에 따라 언어 또는 변이어를 최대한 대표하기 위해 선택된 전자 형태로 되어 있는 언어 텍스트를 모은 집합



### 파이썬으로 말뭉치 전처리하기

```python
import numpy as np
import re


def preprocess(text):
    text = text.lower()
    words = re.split('\W', text)

    word_to_id = {}
    id_to_word = {}

    for word in words:
        if word not in word_to_id:
            new_id = len(word_to_id)
            word_to_id[word] = new_id
            id_to_word[new_id] = word

    corpus = [word_to_id[w] for w in words]
    corpus = np.array(corpus)

    return corpus, word_to_id, id_to_word

text = 'You say goodbye and I say hello.'
print(preprocess(text))
```

![image](https://user-images.githubusercontent.com/52685206/77832937-eb8d2280-717c-11ea-8b0e-342530860639.png)



### 단어의 분산 표현

* 색을 표현하는 방식
  * 코발트블루, 싱크레드 등의 고유 이름
  * RGB(Red/Green/Blue) 를 이용한 3차원 벡터 표현 방식
  * RGB 와 같은 벡터 표현이 색을 더욱 정확하게 표현할 수 있음
  * 비색 vs. (R, G, B) = (170, 33, 22)
  * 색깔 간 관련성(비슷한 색인지 여부 등)도 벡터 표현 쪽이 더 쉽게 판단할 수 있고 정량화하기도 쉬움

* 단어의 의미를 정확하게 파악할 수 있는 벡터 표현은 없을까?
* 이것이 단어의 분산 표현
  * 단어를 고정 길이의 밀집 벡터(dense vector)로 표현
  * 밀집 벡터: 대부분의 원소가 0이 아닌 실수인 벡터



### 분포 가설

* "어휘의 의미는 주변 어휘에 의해 형성된다"
* 분포 가설(distributional hypothesis)
* 어휘 자체에는 의미가 없고 해당 어휘가 사용된 context 에 따라 의미가 형성
* 맥락의 크기(주변 단어를 몇 개나 포함할지): 윈도우 크기(window size)



### 동시 발생 행렬(co-occurrence matrix)

* 단어를 벡터로 나타내는 방법?

  * 주변 단어를 세어 보기
  * 주변에 어떤 어휘가 몇 번 등장했는지
  * 통계 기반(statistical based) 기법

* 모든 어휘에 대해 동시 발생하는 어휘를 행렬의 형태로 표현한 것

* 수동으로 동시 발생 행렬 표현

  ```python
  import numpy as np
  from preprocess import preprocess
  
  text = 'You say goodbye and I say hello.'
  corpus, word_to_id, id_to_word = preprocess(text)
  
  C = np.array([
      [0, 1, 0, 0, 0, 0, 0],
      [1, 0, 1, 0, 1, 1, 0],
      [0, 1, 0, 1, 0, 0, 0],
      [0, 0, 1, 0, 1, 0, 0],
      [0, 1, 0, 1, 0, 0, 0],
      [0, 1, 0, 0, 0, 0, 1],
      [0, 0, 0, 0, 0, 1, 0],
  ], dtype=np.int32)
  
  # ID 가 0 인 벡터 표현
  print(C[0])
  
  # ID 가 4 인 벡터 표현
  print(C[4])
  
  # "goodbye" 의 벡터 표현
  print(C[word_to_id['goodbye']])
  ```

  ![image](https://user-images.githubusercontent.com/52685206/77833172-9f42e200-717e-11ea-82e6-e0fe99854415.png)

* 자동화하기

  ```python
  import numpy as np
  import re
  from a_preprocess import preprocess
  
  
  def create_co_matrix(corpus, vocab_size, window_size=1):
      corpus_size = len(corpus)
      co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)
  
      for idx, word_id in enumerate(corpus):
          for i in range(1, window_size + 1):
              left_idx = idx - i
              right_idx = idx + i
  
              if left_idx >= 0:
                  left_word_id = corpus[left_idx]
                  co_matrix[word_id, left_word_id] += 1
              
              if right_idx < corpus_size:
                  right_word_id = corpus[right_idx]
                  co_matrix[word_id, right_word_id] += 1
      
      return co_matrix
  
  
  text = 'You say goodbye and I say hello.'
  corpus, word_to_id, id_to_word = preprocess(text)
  v_size = len(re.findall('\W', text))
  print(create_co_matrix(corpus, v_size))
  
  ```

  ![image](https://user-images.githubusercontent.com/52685206/77833542-8ab41900-7181-11ea-9ea8-1e2e79e9f910.png)



### 벡터 간 유사도

* 어휘 간 유사도를 나타낼 때는 코사인 유사도(cosine similarity)를 주로 사용함

  ```python
  def cos_similarity(x, y):
      # x 의 정규화
      nx = x / np.sqrt(np.sum(x ** 2))
      # y 의 정규화
      ny = y /np.sqrt(np.sum(y ** 2))
      
      return np.dot(nx, ny)
  ```

  * 문제점: 인수로 제로 벡터(원소가 모두 0 인 벡터)가 들어오면 "0으로 나누기(divide by zeri)" 오류 발생

  * 해결: 나눌 때 분모에 작은 값을 더하기
  * 별도 인수를 받도록 하고, 인수 값을 지정하지 않으면 기본값으로 1e-8(0.00000001) 이 되도록 함

  ```python
  def cos_similarity(x, y, eps=1e-8):
      nx = x / (np.sqrt(np.sum(x ** 2)) + eps)
      ny = y / (np.sqrt(np.sqrt(y ** 2)) + eps)
      
      return np.dot(nx, ny)
  ```

* "you" 와 "i" 의 코사인 유사도 구하기

  ```python
  text = 'You say goodbye and I say hello.'
  corpus, word_to_id, id_to_word = preprocess(text)
  vocab_size = len(word_to_id)
  C = create_co_matrix(corpus, vocab_size)
  
  c0 = C[word_to_id['you']]
  c1 = C[word_to_id['i']]
  print(cos_similarity(c0, c1))
  ```

  ![image](https://user-images.githubusercontent.com/52685206/77833792-4de92180-7183-11ea-9f60-f988d9c17097.png)





### 유사 단어의 랭킹 표시

```python
import numpy as np
from c_02_cos_similarity import preprocess, create_co_matrix, cos_similarity


def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):
    # 검색어 추출
    if query not in word_to_id:
        print(f'{query}(을)를 찾을 수 없습니다.')
        return
    
    print(f'\n[query] {query}')
    query_id = word_to_id[query]
    query_vec = word_matrix[query_id]

    # 코사인 유사도 계산
    vocab_size = len(id_to_word)
    similarity = np.zeros(vocab_size)
    for i in range(vocab_size):
        similarity[i] = cos_similarity(word_matrix[i], query_vec)
    
    # 코사인 유사도를 기준으로 내림차순 출력
    cnt = 0
    for i in (-1 * similarity).argsort():
        if id_to_word[i] == query: continue
        print(f' {id_to_word[i]}: {similarity[i]}')

        cnt += 1
        if cnt >= top: return


text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)
vocab_size = len(word_to_id)
C = create_co_matrix(corpus, vocab_size)

most_similar('you', word_to_id, id_to_word, C, top=5)
```

![image](https://user-images.githubusercontent.com/52685206/77833998-c4d2ea00-7184-11ea-81f3-786ff88a7ab1.png)





## 통계 기반 기법 개선하기

### 상호 정보량

* "the" 와 "car" 의 동시 발생과 "car" 와 "drive" 의 동시 발생

  * 빈도를 기준으로 보면 "the" 가 고빈도 어휘이므로 "drive"보다 "car" 와 더 강한 관련성을 갖게 되는 것으로 평가

* 위의 문제를 해결하기 위하여 점별 상호 정보량(Pointwise Mutual Information; PMI) 사용

  * 단어가 단독으로 출현하는 횟수가 고려됨
  * 하지만 두 단어의 동시 발생 횟수가 0인 경우 log₂0 = -∞ 이 되는 문제 발생

* 이러한 문제를 피하기 위하여 실제 구현 시에는 양의 상호 정보량(Positive PMI; PPMI) 을 사용

  ```python
  import numpy as np
  
  # verbose: 진행 상황 출력 여부를 결정할 flag
  def ppmi(C, verbose=False, eps=1e-8):
      M = np.zeros_like(C, dtype=np.float32)
      N = np.sum(C)
      S = np.sum(C, axis=0)
      total = C.shape[0] * C.shape[1]
      cnt = 0
  
      for i in range(C.shape[0]):
          for j in range(C.shape[1]):
              pmi = np.log2(C[i, j] * N / (S[j] * S[i]) + eps)
              M[i, j] = max(0, pmi)
  
              if verbose:
                  cnt += 1
                  if cnt % (total // 100) == 0:
                      print(f'{100 * cnt / total:#.1f}% 완료')
      
      return M
  
  
  text = 'You say goodbye and I say hello.'
  corpus, word_to_id, id_to_word = preprocess(text)
  vocab_size = len(word_to_id)
  C = create_co_matrix(corpus, vocab_size)
  W = ppmi(C)
  
  np.set_printoptions(precision=3)
  print('동시 발생 행렬')
  print(C)
  print('-' * 50)
  print('PPMI')
  print(W)
  ```

  ![image](https://user-images.githubusercontent.com/52685206/77842793-8fa0b900-71d1-11ea-8f60-28979518e059.png)
  * 말뭉치 어휘 수가 증가함에 따라 각 단어 벡터의 차원 수도 증가하는 문제 존재
  * 행렬의 원소 대부분이 0 임; 벡터 원소 대부분이 중요하지 않음
  * 이러한 벡터는 노이즈에 약하고 견고하지 못함
  * 벡터의 차원 감소 기법 사용



### 차원 감소(dimentionality reduction)

* 벡터의 차원을 줄이는 방법

* '중요한 정보' 는 최대한 유지하면서 줄이는 것이 핵심!

* 데이터의 분포를 고려해 중요한 '축' 을 찾는 일을 수행

* 중요한 것은 가장 적합한 축을 찾아내는 일로, 1차원 값만으로더 데이터의 본질적인 차이를 구별할 수 있어야 함

* 특잇값 분해(Singular Value Decomposition; SVD) 를 사용

  * 임의의 행렬을 세 행렬의 곱으로 분해
  * X =  USV<sup>T</sup>
  * U 와 V 는 직교 행렬(orthogonal matrix), S 는 대각 행렬(diagonal matrix; 대각 성분 외에는 모두 0 인 행렬)
  * U(직교 행렬): 어떠한 공간의 축(기저)을 형성, 단어 공간으로 볼 수 있음
  * S(대각 행렬): 특잇값(singular value; 해당 축의 중요도)이 큰 순서로 나열, 이후 중요도가 낮은 원소를 깎아냄

  ```python
  text = 'You say goodbye and I say hello.'
  corpus, word_to_id, id_to_word = preprocess(text)
  vocab_size = len(word_to_id)
  C = create_co_matrix(corpus, vocab_size, window_size=1)
  W = ppmi(C)
  
  # SVD
  U, S, V = np.linalg.svd(W)
  
  # 단어 id 가 0 인 단어 벡터 확인
  # 동시 발생 행렬
  print(f'동시 발생 행렬: {C[0]}')
  
  # PPMI 행렬
  print(f'PPMI 행렬: {W[0]}')
  
  # SVD
  print(f'SVD: {U[0]}')
  
  
  # 밀집 벡터의 차원을 2차원 벡터로 감소
  print('2차원 벡터')
  print(U[0, :2])
  
  # 그래프로 그리기
  for word, word_id in word_to_id.items():
      plt.annotate(word, (U[word_id, 1], U[word_id, 0]))
  
  plt.scatter(U[:, 1], U[:, 0], alpha=0.5)
  plt.show()
  ```

  ![image](https://user-images.githubusercontent.com/52685206/77845593-17df8800-71eb-11ea-82e7-c733c7a2ec9e.png)

  ![result](https://user-images.githubusercontent.com/52685206/77845604-2af25800-71eb-11ea-9e82-60f798aa94d5.png)



### PTB(펜 트리뱅크; Penn Treebank) 데이터셋

* PTB corpus 는 word2vec 의 발명자인 토마스 미콜로프의 웹 페이지에서 다운로드 가능





### PTB 데이터셋 평가

```python
window_size = 2
wordvec_size = 100

corpus, word_to_id, id_to_word = ptb.load_data('train')
vocab_size = len(word_to_id)
print('동시발생 수 계산 ...')
C = create_co_matrix(corpus, vocab_size, window_size)
print('PPMI 계산 ...')
W = ppmi(C, verbose=True)

print('calculating SVD ...')
try:
    # truncated SVD (빠르다!)
    from sklearn.utils.extmath import randomized_svd
    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,
                             random_state=None)
except ImportError:
    # SVD (느리다)
    U, S, V = np.linalg.svd(W)

word_vecs = U[:, :wordvec_size]

querys = ['you', 'year', 'car', 'toyota']
for query in querys:
    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)
```

* `randomized_svd()`

  * 무작위 수를 사용한 Truncated SVD

  * 특잇값이 큰 것들만 계산

  * Truncated SVD 는 무작위 수를 사용하므로 결과가 매번 달라짐

    ![image](https://user-images.githubusercontent.com/52685206/77848937-06a27580-7203-11ea-9184-a95fe43c07ab.png)

  * 단어의 의미 또는 문법적 관점에서 유사한 단어들이 가까운 벡터로 나타남

